{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMeOsca0YnCs6H3tdG9mmNN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aryu-Tamura/GoogleColab_RAG_test/blob/main/RAG%E3%82%92%E7%94%A8%E3%81%84%E3%81%9FLLM%E3%81%AE%E5%87%BA%E5%8A%9B%E6%A4%9C%E8%A8%BC%E3%83%AC%E3%83%9D%E3%83%BC%E3%83%88.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**RAGを用いたLLMの出力検証レポート**\n",
        "\n",
        "###**（1）やったこと・動かしたものの概要**\n",
        "\n",
        "RAGを実装し、出力の精度について考察した。具体的には、\"RAGを実装した場合\"、\"追加データなしで使用したLLMモデルに依存する場合”、\"RAGで分割する前のテキストデータをそのままプロンプトとして入力した場合\"、のそれぞれで出力結果の比較を行った。\n",
        "\n"
      ],
      "metadata": {
        "id": "LaaS9FNlf1Mb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**（2）具体的な手続き**\n",
        "\n",
        "Google Colab上で以下のプログラムを実行した。コーディングはGeminiを活用して行った。\n",
        "\n",
        "きっかけ：\n",
        "松尾研LLMコミュニティ【Paper&Hacks #43】の井伊篤彦氏による「RAG vs ロングコンテクストアプリ・ハンズオン」に参加したこと。\n",
        "\n"
      ],
      "metadata": {
        "id": "juolBPtZjibP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_cuO7AwdPiAy"
      },
      "outputs": [],
      "source": [
        "# ライブラリのインストール\n",
        "!pip install openai\n",
        "!pip install langchain\n",
        "!pip install chromadb\n",
        "!pip install tiktoken\n",
        "!pip install -U langchain-community\n",
        "!pip install langchain-openai\n",
        "\n",
        "# ライブラリのインポート\n",
        "import os\n",
        "from google.colab import files\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_openai import ChatOpenAI\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI API ログイン\n",
        "from google.colab import userdata\n",
        "import openai\n",
        "\n",
        "key = userdata.get('OpenAI_key')\n",
        "os.environ[\"OPENAI_API_KEY\"] = key\n",
        "openai.api_key = key\n",
        "client = OpenAI()\n",
        "\n",
        "print(\"OpenAI APIキーが設定されました。\")"
      ],
      "metadata": {
        "id": "wBvVzkNZQOA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wikipediaから取得した「ONE PIECE」に関する約1万7000文字のテキストデータアップロードしている。データの整理などは行っていない。"
      ],
      "metadata": {
        "id": "mOVhWvHoikB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ファイルアップロード\n",
        "uploaded = files.upload()\n",
        "file_name = next(iter(uploaded))\n",
        "raw_text = uploaded[file_name].decode('utf-8')\n",
        "print(f\"\\nアップロードされたファイル '{file_name}' の内容 (先頭300文字):\\n{raw_text[:300]}...\\n\")"
      ],
      "metadata": {
        "id": "pmwlyqYeTPox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 質問リスト\n",
        "questions = [\n",
        "    \"アニメ『ONE PIECE』の放送開始当初の放送曜日は何曜日でしたか？ \",\n",
        "    \"アニメ『ONE PIECE』で、2023年12月に制作が発表された、原作を「東の海編」から再アニメ化する新アニメシリーズのタイトルは何ですか？ \",\n",
        "    \"アニメ『ONE PIECE』で、麦わらの一味の声優が他のキャラクターの声を兼任する際に使われる別名義の冒頭の名称は何ですか？\",\n",
        "    \"アニメ『ONE PIECE』が放送1000話に到達したのは西暦何年ですか？ \"\n",
        "]\n",
        "\n",
        "# RAGありで質問応答を行う関数\n",
        "def rag_based_qa(question, retriever, llm):\n",
        "    qa = RetrievalQA.from_llm(llm=llm, retriever=retriever, return_source_documents=True)\n",
        "    result = qa({\"query\": question})\n",
        "    print(f\"\\n質問: {question}\")\n",
        "    print(\"回答 (RAGあり):\", result[\"result\"])\n",
        "    print(\"参照元ドキュメント:\")\n",
        "    for doc in result[\"source_documents\"]:\n",
        "        print(f\"  {doc.page_content[:100]}...\")\n",
        "    return result[\"result\"]\n",
        "\n",
        "# RAGなしで質問応答を行う関数\n",
        "def no_rag_qa(question, llm):\n",
        "    response = client.chat.completions.create(\n",
        "        model=llm.model_name,\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": question}\n",
        "        ]\n",
        "    )\n",
        "    print(f\"\\n質問: {question}\")\n",
        "    print(\"回答 (RAGなし):\", response.choices[0].message.content)\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# RAGなしでテキスト全体をプロンプトに含めて質問応答を行う関数\n",
        "def no_rag_with_context_qa(question, text, llm):\n",
        "    prompt_with_context = f\"以下のテキストに基づいて質問に答えてください。\\n\\n{text}\\n\\n質問: {question}\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=llm.model_name,\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt_with_context}\n",
        "        ]\n",
        "    )\n",
        "    print(f\"\\n質問: {question}\")\n",
        "    print(\"回答 (RAGなし/全文):\", response.choices[0].message.content)\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# テキスト分割\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "documents = text_splitter.create_documents([raw_text])\n",
        "\n",
        "# Embeddingモデルの初期化\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# ベクトルストアの構築\n",
        "db = Chroma.from_documents(documents, embeddings)\n",
        "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# LLMの初期化 (gpt-4o-mini を使用)\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
        "\n",
        "print(\"--- RAG あり ---\")\n",
        "rag_results = {}\n",
        "for question in questions:\n",
        "    rag_results[question] = rag_based_qa(question, retriever, llm)\n",
        "\n",
        "print(\"\\n--- RAG なし ---\")\n",
        "no_rag_results = {}\n",
        "for question in questions:\n",
        "    no_rag_results[question] = no_rag_qa(question, llm)\n",
        "\n",
        "print(\"\\n--- RAG なし (テキスト全体をプロンプトに含めて与える) ---\")\n",
        "no_rag_context_results = {}\n",
        "for question in questions:\n",
        "    no_rag_context_results[question] = no_rag_with_context_qa(question, raw_text, llm)\n",
        "\n",
        "print(\"\\n--- 結果比較 ---\")\n",
        "for question in questions:\n",
        "    print(f\"\\n質問: {question}\")\n",
        "    print(f\"  回答 (RAGあり): {rag_results[question]}\")\n",
        "    print(f\"  回答 (RAGなし): {no_rag_results[question]}\")\n",
        "    print(f\"  回答 (RAGなし/全文): {no_rag_context_results[question]}\")"
      ],
      "metadata": {
        "id": "DUufCPKuTe-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pandasで結果をわかりやすく可視化する。"
      ],
      "metadata": {
        "id": "chZiiYEVjaT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# 結果データ（再掲）\n",
        "results_data = [\n",
        "    {\"質問\": \"アニメ『ONE PIECE』の放送開始当初の放送曜日は何曜日でしたか？\", \"正解\": \"水曜日\", \"結果\": \"○/○/○\"},\n",
        "    {\"質問\": \"アニメ『ONE PIECE』で、2023年12月に制作が発表された、原作を「東の海編」から再アニメ化する新アニメシリーズのタイトルは何ですか？\", \"正解\": \"『THE ONE PIECE』\", \"結果\": \"○/H/○\"},\n",
        "    {\"質問\": \"アニメ『ONE PIECE』で、麦わらの一味の声優が他のキャラクターの声を兼任する際に使われる別名義の冒頭の名称は何ですか？\", \"正解\": \"粗忽屋（そこつや）\", \"結果\": \"×/H/○\"},\n",
        "    {\"質問\": \"アニメ『ONE PIECE』が放送1000話に到達したのは西暦何年ですか？\", \"正解\": \"2021年\", \"結果\": \"○/○/○\"}\n",
        "]\n",
        "\n",
        "# pandas DataFrameの作成\n",
        "df = pd.DataFrame(results_data)\n",
        "df.columns = [\"質問\", \"正解\", \"結果 [A]/[B]/[C]\"]\n",
        "\n",
        "# DataFrameをHTMLとして表示（より綺麗で見やすい）\n",
        "display(HTML(df.to_html(index=False)))\n",
        "\n",
        "# 注意書き\n",
        "print(\"\\n--- 注意 ---\")\n",
        "print(\"結果の表記:\")\n",
        "print(\"○: 正解\")\n",
        "print(\"×: 回答なし\")\n",
        "print(\"H: ハルシネーション（事実に基づかない内容を含む回答）\")\n",
        "print(\"[A]: アップロードされたデータから検索（RAG）\")\n",
        "print(\"[B]: gpt-4o-miniにデータを与えずに直接質問\")\n",
        "print(\"[C]: アップロードされたデータ全体を入力\")"
      ],
      "metadata": {
        "id": "SHRStih1bP8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**（3）わかったこと・わからなかったこと**\n"
      ],
      "metadata": {
        "id": "DJFVbM26sZkE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**わかったこと**\n",
        "\n",
        "\n",
        "RAGを実装したLLMの出力結果を分析したところ、質問3において「情報が提供されていない」という回答が得られた。正解となる情報自体はシステムに提供していたにも関わらず、このような出力になった点は、一見すると期待外れかもしれない。しかし、特筆すべきは、この状況下でLLMがハルシネーション（事実に基づかない情報生成）を起こさなかったという点である。これは、エンタープライズ環境、特にコールセンターのような正確性が求められるチャットボット開発においては非常に重要な特性と言える。誤った情報を出力するリスクを最小限に抑え、確実な情報のみを提供する姿勢は、RAGの大きな利点であると考えらる。一方、追加データなしでLLMに直接質問した場合、4問中2問でハルシネーションが発生しており、RAGが情報の信頼性を高める上で有効であることがわかった。\n",
        "\n",
        "\n",
        "興味深い結果として、RAGとしてではなく、アップロードしたデータの全文をプロンプトとしてLLMに与えた場合、正答率は100%を達成した。近年のLLMはコンテキストウィンドウ（一度に入力可能なプロンプトの量）が飛躍的に拡大しており、このアプローチも現実的な選択肢となりつつあると考えらる。しかしながら、入力するデータが極めて大規模になるケースでは、この手法は入力トークン数の上限を超える可能性があり、適用が難しい場面も想定される。\n",
        "\n",
        "\n",
        "そこで、RAGと全文入力を組み合わせたハイブリッドなアプローチの可能性を感じた。例えば、行政の補助金・助成金案内のチャットボットを想定する。多岐にわたる助成金情報を全てプロンプトに含めるのは非現実的である。しかし、ユーザーが自身の属性やニーズをチェックマークなどで選択し、関連性の高い情報群を絞り込むことができれば、その絞り込まれた比較的小規模なデータをプロンプトの全文としてLLMに提供することが可能である。この手法であれば、大規模なデータ全体を扱うことなく、ユーザーが必要とする情報を高い精度でLLMから引き出せるのではないかと考えた。\n"
      ],
      "metadata": {
        "id": "Vxr40jtgsT5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**わからなかったこと**\n",
        "\n",
        "\n",
        "今回の検証では、アップロードしたデータの整理や前処理をほとんど行わなかった。文献調査などを通して、より効果的なデータのクリーニングや構造化の手法を学び、それをRAGに適用することで、LLMの出力精度が向上する可能性は大いにあると考えられる。今後は、関連する論文を読み解き、データ整理の実装に挑戦していきたい。\n",
        "\n",
        "\n",
        "また、今回の検証で使用したLLMはgpt-4o-miniのみだった。OpenAI以外の主要なLLM、例えばGoogleのGeminiやAnthropicのClaudeといったモデルでも同様の実験を行い、それぞれの特性やRAGとの相性を比較することで、より深い知見が得られると考えられる。異なるモデルでの挙動を把握することは、実際のアプリケーション開発において最適なモデルを選択する上で不可欠なステップだと考えられる。"
      ],
      "metadata": {
        "id": "31z0LZ-EgTwA"
      }
    }
  ]
}